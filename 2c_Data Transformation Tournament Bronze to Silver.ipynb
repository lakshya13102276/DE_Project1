{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02b578b6-9d81-4e19-b60d-364d6dbabee6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Transformation Level 1 \n",
    "##### Transforming data in Tournament Directory to dataframe and adding ingestion date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57791da8-9c4b-4744-b38f-3678be90b28e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Defining the schema ourself for the bating Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a209cb30-1bdf-4c7a-a436-2bd5aacc0680",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType ,StructField,IntegerType,DoubleType,StringType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60ef16e7-9428-4dea-86c0-5cccf3362a90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema_tour = StructType([\n",
    "    StructField(\"final_id\", IntegerType(), False),\n",
    "    StructField(\"date_final\", StringType(), False),\n",
    "    StructField(\"nbr_teams\", IntegerType(), True),\n",
    "    StructField(\"game_format\", StringType(), True),\n",
    "    StructField(\"venue\", StringType(), True),\n",
    "    StructField(\"venue_choice\", StringType(), True),\n",
    "    StructField(\"winner\", StringType(), True),\n",
    "    StructField(\"runner_up\", StringType(), True),\n",
    "    StructField(\"toss_win\", StringType(), True),\n",
    "    StructField(\"bowling_select\", StringType(), True),\n",
    "    StructField(\"player_top_scorer\", StringType(), True),\n",
    "    StructField(\"player_top_wkts\",StringType(), True),\n",
    "    StructField(\"player_series\", StringType(), True),\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2de8481f-d5f4-48f7-9f4b-b15150d96ea6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_tour_raw = spark.read.csv(\"/mnt/saprojectone/bronze-project1/tour\",header=\"True\", schema=schema_tour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3c824b1-b4f4-4d5f-aa29-2381fca41ccb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Adding Ingestion date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae643673-8b60-41a2-85c2-7f021b8a116b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2396e1f-1a81-4e5b-8bae-8729b5519564",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_tour_t1 = df_tour_raw.withColumn(\"ingestion_date\",current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "946042d9-3ac7-4f9d-9948-fb5f8a74f823",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, to_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a2f554b-4efe-4f32-8dc0-fe0b591af9a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Need to add the below statement as the type of datetime in the raw data is not recognised in the new spark versions. Othherwise use regex to remove the day from the data and then call to_date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "385fd77a-1bde-408b-8944-e931fd70dce0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66d4a491-a8c0-4f6a-96a5-6651fa30cd04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_tour_t2 = df_tour_t1.withColumn(\"date_final\", to_date(df_tour_t1[\"date_final\"], \"EEEE, MMMM dd, yyyy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3665bcc-8ed8-43ac-9ec7-aedf3e289200",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "##### Copying the data in the Silver container for 2nd level of transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dda3d25-7eba-4c30-abe2-f77f2f703e5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_tour_t2.write.mode(\"overwrite\").format(\"delta\").save(\"/mnt/saprojectone/silver-project1/tour\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2c_Data Transformation Tournament Bronze to Silver",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
